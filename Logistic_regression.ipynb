{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (using GloVe)\n",
    "The idea behind this approach is to average the word vectors over every tweet, and use this average vectors to train logistic regression. <br> Cells that need user input have a <font color='blue'> blue title</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from logreg import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Define file paths</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to embeddings\n",
    "path_embeddings = 'embeddings/embeddings200.npy'\n",
    "# path to tweets\n",
    "path_train_pos = 'data/train_pos.txt'\n",
    "path_train_neg = 'data/train_neg.txt'\n",
    "# filename of the submission file\n",
    "submission_filename = 'logreg_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Define hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 1000\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.01\n",
    "print_every = int(50000 / batch_size)\n",
    "\n",
    "#chose wheter to use offset, standardization\n",
    "standardize = False\n",
    "offset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our GloVe word embeddings from file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(path_embeddings)\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word vectors over tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000  tweets processed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Appending a row for each tweet is impracticable slow. \n",
    "However, we can not know in advance the number of tweets we will\n",
    "be appended (this is because we skip tweets fr which we have no embeddings).\n",
    "therefore we allocate a too big array fr x_train and cut wht's too much\n",
    "in the end.\n",
    "'''\n",
    "\n",
    "# Process training tweets\n",
    "allocate_columns = 3000000\n",
    "x_train = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "y_train = np.zeros(allocate_columns)\n",
    "counter = 0\n",
    "\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = 1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_train[counter, :] = mean\n",
    "            y_train[counter] = -1\n",
    "            counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "# cut zero rows in x_train and y_train\n",
    "y_train = y_train[np.nonzero(y_train)]\n",
    "x_train = x_train[np.nonzero(y_train)]\n",
    "                               \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000  tweets processed\n",
      "10000  tweets processed\n"
     ]
    }
   ],
   "source": [
    "# Process test tweets\n",
    "allocate_columns = 100000\n",
    "x_submission = np.zeros((allocate_columns, embeddings.shape[1]))\n",
    "embeddings_mean = np.expand_dims(np.mean(embeddings, axis=0), axis=0)\n",
    "counter = 0\n",
    "\n",
    "with open('test_data.txt') as f:\n",
    "    for line in f:\n",
    "        total = np.zeros((1, embeddings.shape[1]))\n",
    "        wordcount = 0\n",
    "        # filter out the IDs and first comma\n",
    "        tweet = line[(line.index(\",\")+1):]\n",
    "        for word in tweet.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                total += embeddings[index, :]\n",
    "                wordcount += 1\n",
    "        if(wordcount > 0):\n",
    "            mean = total / wordcount\n",
    "            x_submission[counter, :] = mean\n",
    "        else:\n",
    "            # in case that we have no embedding for any word of the tweet\n",
    "            # just use the overall mean of the embeddings\n",
    "            x_submission[counter, :] = embeddings_mean\n",
    "        counter += 1\n",
    "        if counter % 5000 == 0:\n",
    "            print(str(counter), \" tweets processed\")\n",
    "            \n",
    "# cut zero rows in x_submission\n",
    "x_submission = x_submission[np.nonzero(x_submission[:, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t 1 \tloss:  853.4164276410547\n",
      "epoch\t 1 \tloss:  795.5189160441892\n",
      "epoch\t 1 \tloss:  789.7773493339978\n",
      "epoch\t 2 \tloss:  779.3013834510737\n",
      "epoch\t 2 \tloss:  778.2169460365299\n",
      "epoch\t 2 \tloss:  768.5239878244789\n",
      "epoch\t 3 \tloss:  759.6393048285314\n",
      "epoch\t 3 \tloss:  758.4506480348911\n",
      "epoch\t 3 \tloss:  754.7865378106121\n",
      "epoch\t 4 \tloss:  745.6170809548775\n",
      "epoch\t 4 \tloss:  741.1574477338334\n",
      "epoch\t 4 \tloss:  741.8580200421977\n",
      "epoch\t 5 \tloss:  732.6044712091044\n",
      "epoch\t 5 \tloss:  731.2572481762014\n",
      "epoch\t 5 \tloss:  728.2809328913614\n",
      "epoch\t 6 \tloss:  721.42103260051\n",
      "epoch\t 6 \tloss:  718.0030269014238\n",
      "epoch\t 6 \tloss:  717.3540687766836\n",
      "epoch\t 7 \tloss:  712.257444169504\n",
      "epoch\t 7 \tloss:  707.3484109267555\n",
      "epoch\t 7 \tloss:  708.3399215215265\n",
      "epoch\t 8 \tloss:  701.8345628840065\n",
      "epoch\t 8 \tloss:  702.3227469942924\n",
      "epoch\t 8 \tloss:  701.4123006237901\n",
      "epoch\t 9 \tloss:  695.2529400678187\n",
      "epoch\t 9 \tloss:  693.3851979820482\n",
      "epoch\t 9 \tloss:  692.8900604718871\n",
      "epoch\t 10 \tloss:  689.367144046254\n",
      "epoch\t 10 \tloss:  684.3950780208949\n",
      "epoch\t 10 \tloss:  687.6861738706434\n",
      "epoch\t 11 \tloss:  685.4390043552688\n",
      "epoch\t 11 \tloss:  680.426423197137\n",
      "epoch\t 11 \tloss:  680.8257907042005\n",
      "epoch\t 12 \tloss:  679.486331977657\n",
      "epoch\t 12 \tloss:  677.1767056573058\n",
      "epoch\t 12 \tloss:  677.9885534742072\n",
      "epoch\t 13 \tloss:  672.7474488157666\n",
      "epoch\t 13 \tloss:  674.8139711568675\n",
      "epoch\t 13 \tloss:  673.2374112968967\n",
      "epoch\t 14 \tloss:  670.318262504696\n",
      "epoch\t 14 \tloss:  667.2261072570801\n",
      "epoch\t 14 \tloss:  668.6097060387807\n"
     ]
    }
   ],
   "source": [
    "# set aside a small portion for validation\n",
    "testset = 10000\n",
    "\n",
    "x_test = x_train[0:testset, :]\n",
    "y_test = y_train[0:testset]\n",
    "x_train_log = x_train[testset + 1:, :]\n",
    "y_train_log = y_train[testset + 1:]\n",
    "\n",
    "if standardize == True:\n",
    "    x_train_log, mean, std = standardize(x_train_log)\n",
    "\n",
    "if offset == True:\n",
    "    x_train_log = add_offset(x_train_log)\n",
    "\n",
    "# train using logistic regression (SGD)\n",
    "initial_w = np.random.rand(x_train_log.shape[1])\n",
    "weights, loss = reg_logistic_regression(y_train_log, x_train_log, initial_w, epochs, batch_size, gamma, lambda_, print_every)\n",
    "\n",
    "# free up memory\n",
    "del x_train_log\n",
    "del y_train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on a local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardize == True:\n",
    "    x_test_log = standardize_test(x_test, mean, std)\n",
    "\n",
    "if offset == True:\n",
    "    x_test_log = add_offset(x_test)\n",
    "\n",
    "y_pred = predict_logistic_labels(weights, x_test_log)\n",
    "accuracy = get_accuracy(y_test, y_pred)\n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test dataset, prepare submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardize == True:\n",
    "    x_submission_log = standardize_test(x_submission_log, mean, std)\n",
    "\n",
    "if offset == True:\n",
    "    x_submission_log = add_offset(x_submission)\n",
    "\n",
    "y_submission = predict_logistic_labels(weights, x_submission_log)\n",
    "\n",
    "# we need to add IDs to meet the submission interface requirements\n",
    "ids = np.arange(len(y_submission)) + 1\n",
    "create_csv_submission(ids, y_submission, submission_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
