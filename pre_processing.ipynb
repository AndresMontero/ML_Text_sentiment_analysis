{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from pre_processing import *\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Set....\n",
      "Data Set loaded !\n"
     ]
    }
   ],
   "source": [
    "#Load Data Sets\n",
    "DATA_FOLDER = './data'\n",
    "\n",
    "tweets_col_names=['text']\n",
    "\n",
    "tweets_dtypes = {'text': str }\n",
    "\n",
    "print(\"Loading Data Set....\")\n",
    "tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n",
    "                                  dtype=tweets_dtypes)\n",
    "tweets_test = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n",
    "tweets_test.drop(columns=[1,2], inplace=True)\n",
    "tweets_test.rename(columns={0:'text'}, inplace= True)\n",
    "\n",
    "    \n",
    "print(\"Data Set loaded !\")\n",
    "\n",
    "\n",
    "#bool variables to clean data\n",
    "#If set to False the cleaning process will take place\n",
    "already_cleaned_neg = False\n",
    "already_cleaned_pos = False\n",
    "already_cleaned_neg_full = False\n",
    "already_cleaned_pos_full = False\n",
    "already_cleaned_test = False\n",
    "\n",
    "#variables to define which functions to apply \n",
    "#to the clean_data function\n",
    "duplicates = True\n",
    "emojis = False\n",
    "punctuaction = False\n",
    "handle_number = False\n",
    "special_symbols = True\n",
    "moreLetters = False\n",
    "contractions = False\n",
    "clean_stopwords = False\n",
    "spelling = False\n",
    "lemmatize = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"Function to clean the data.\n",
    "\n",
    "    Args:\n",
    "        data: data to clean\n",
    "        \n",
    "    Returns:\n",
    "        data cleaned\n",
    "    \"\"\"\n",
    "    #increase index to start at 1\n",
    "    data.index = data.index +1\n",
    "    \n",
    "    #remove duplicates\n",
    "    if duplicates:\n",
    "        print(\"removing duplicates!\")\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        \n",
    "    if emojis:\n",
    "        print(\"emoji\")\n",
    "        data['text'] = data['text'].apply(lambda x: interpret_emoji(x))\n",
    "    \n",
    "    if punctuaction:\n",
    "        print(\"punctuaction!\")\n",
    "        #remove punctuaction .........\n",
    "        data['text'] = data['text'].str.replace('.','')\n",
    "        data['text'] = data['text'].str.replace(',','')\n",
    "        \n",
    "    if handle_number:\n",
    "        print(\"handling numbers!!\")\n",
    "        #separate number with letters  1234test123 =>  test \n",
    "        data['text'] = data['text'].str.replace('[0-9]+',' <number> ')\n",
    "    \n",
    "    if special_symbols:\n",
    "        print(\"special symbols!!\")\n",
    "        data['text'] = data['text'].str.replace('[();=\\\\/:*?\\|&]+','')\n",
    "        \n",
    "    if moreLetters:\n",
    "        print(\"more letters!\")\n",
    "        data['text'] = data['text'].apply(lambda x: replace_moreletters(x))\n",
    "    \n",
    "    if contractions:\n",
    "        print(\"expanding contractions!\")\n",
    "        data['text'] = data['text'].apply(lambda x: expand_contractions(x))\n",
    "\n",
    "    if clean_stopwords:\n",
    "        print(\"removing stop words!\")\n",
    "        data['text'] = data['text'].apply(lambda x: \" \".join\\\n",
    "                                          (x for x in x.split() if x not in stop))\n",
    "    if spelling:\n",
    "        print(\"correcting spelling!\")\n",
    "        data['text'] = data['text'].apply(lambda x: ''.join\\\n",
    "                                          (TextBlob(x).correct()))\n",
    "    if lemmatize:\n",
    "        print(\"lemmatizing!\")\n",
    "        w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        stop = stopwords.words('english')\n",
    "        def lemmatize_text(text):\n",
    "            return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "        \n",
    "        data['text_lema'] = data['text'].apply(lemmatize_text)\n",
    "        data['text_lema'] = data['text_lema'].apply(lambda x: ' '.join(x))\n",
    "        data['text_lema'] = data['text_lema'].str.replace(',','')\n",
    "\n",
    "        \n",
    "    print(\"one space!\")\n",
    "    data['text'] = data['text'].apply(lambda x: one_space(x)) #ok \n",
    "    \n",
    "    \n",
    "    \n",
    "    return  data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Neg\n",
      "removing duplicates!\n",
      "special symbols!!\n",
      "one space!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack 6 difficulty 10 of 10 object disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
       "2    glad i dot have taks tomorrow ! ! #thankful #startho                                                                  \n",
       "3    1-3 vs celtics in the regular season were fucked if we play them in the playoffs                                      \n",
       "4    <user> i could actually kill that girl i'm so sorry ! ! !                                                             \n",
       "5    <user> <user> <user> i find that very hard to believe im afraid                                                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_neg :\n",
    "    print(\"Cleaning Tweets Neg\")\n",
    "    tweets_neg.drop_duplicates(inplace=True)\n",
    "    clean_data(tweets_neg)\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_cleaned.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_neg['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_neg['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_neg_cleaned.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Pos Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_cleaned.pickle\",\"rb\")\n",
    "tweets_neg_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_pickle.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Pos\n",
      "removing duplicates!\n",
      "special symbols!!\n",
      "one space!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    <user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
       "2    because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>                                    \n",
       "3    \" <user> just put casper in a box ! \" looved the battle ! #crakkbitch                                                          \n",
       "4    <user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !                                                 \n",
       "5    visiting my brother tmr is the bestest birthday gift eveerrr ! ! !                                                             \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_pos :\n",
    "    print(\"Cleaning Tweets Pos\")\n",
    "    tweets_pos.drop_duplicates(inplace=True)\n",
    "    clean_data(tweets_pos)\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_cleaned.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_pos['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_pos['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_pos_cleaned.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Neg Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_cleaned.pickle\",\"rb\")\n",
    "tweets_pos_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Neg Full\n",
      "removing duplicates!\n",
      "special symbols!!\n",
      "one space!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    vinco tresorpack 6 difficulty 10 of 10 object disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
       "2    glad i dot have taks tomorrow ! ! #thankful #startho                                                                  \n",
       "3    1-3 vs celtics in the regular season were fucked if we play them in the playoffs                                      \n",
       "4    <user> i could actually kill that girl i'm so sorry ! ! !                                                             \n",
       "5    <user> <user> <user> i find that very hard to believe im afraid                                                       \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_neg_full :\n",
    "    print(\"Cleaning Tweets Neg Full\")\n",
    "    tweets_neg_full.drop_duplicates(inplace=True)\n",
    "    clean_data(tweets_neg_full)\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_neg_full_cleaned.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_neg_full['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_neg_full_cleaned.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Neg Full Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_neg_full_cleaned.pickle\",\"rb\")\n",
    "tweets_neg_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_neg_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Tweets Pos Full\n",
      "removing duplicates!\n",
      "special symbols!!\n",
      "one space!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    <user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
       "2    because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>                                    \n",
       "3    \" <user> just put casper in a box ! \" looved the battle ! #crakkbitch                                                          \n",
       "4    <user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !                                                 \n",
       "5    visiting my brother tmr is the bestest birthday gift eveerrr ! ! !                                                             \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_pos_full :\n",
    "    print(\"Cleaning Tweets Pos Full\")\n",
    "    tweets_pos_full.drop_duplicates(inplace=True)\n",
    "    clean_data(tweets_pos_full)\n",
    "    #save the file to pickle\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    pickle_out = open(\"data/pre_processed/tweets_pos_full_cleaned.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_pos_full['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save file to text\n",
    "    f = open(\"data/pre_processed/tweets_pos_full_cleaned.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text'])))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Tweets Pos Full Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/tweets_pos_full_cleaned.pickle\",\"rb\")\n",
    "tweets_pos_full_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_pos_full_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Test\n",
      "removing duplicates!\n",
      "special symbols!!\n",
      "one space!\n",
      "Saving file with preprocessed Tweets\n",
      "Saved!\n",
      "Opening pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    sea doo pro sea scooter sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>     \n",
       "3    <user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
       "4    i cant stay away from bug thats my baby                                                                                  \n",
       "5    <user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao                                              \n",
       "6    whenever i fall asleep watching the tv , i always wake up with a headache                                                \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not already_cleaned_test :\n",
    "    tweets_test.index = tweets_test.index +1\n",
    "    tweets_test['text'] = tweets_test['text'].str.replace('\\d+,', '')\n",
    "    print(\"Cleaning Test\")\n",
    "    clean_data(tweets_test).head()\n",
    "    print(\"Saving file with preprocessed Tweets\")\n",
    "    #save to pickle\n",
    "    pickle_out = open(\"data/pre_processed/test_data_cleaned.pickle\",\"wb\")\n",
    "    if not lemmatize:\n",
    "        pickle.dump(tweets_test['text'], pickle_out)\n",
    "    else:\n",
    "        pickle.dump(tweets_test['text_lema'], pickle_out)\n",
    "    pickle_out.close()\n",
    "    #save to txt\n",
    "    f = open(\"data/pre_processed/test_data_cleaned.txt\", \"w\", encoding='utf-8')\n",
    "    if not lemmatize:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text']))))\n",
    "    else:\n",
    "        f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(tweets_test['text_lema']))))\n",
    "    f.close()\n",
    "    print(\"Saved!\")\n",
    "else:\n",
    "    print(\"Test Already Cleaned!\")\n",
    "pickle_in = open(\"data/pre_processed/test_data_cleaned.pickle\",\"rb\")\n",
    "tweets_test_pickle = pickle.load(pickle_in)\n",
    "print(\"Opening pickle\")\n",
    "tweets_test_pickle.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
