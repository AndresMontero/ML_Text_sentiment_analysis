{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by Convolutional Neural Nets (CNN)\n",
    "Inspired by [Bentrevett - ConvolutionalSentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb) and the exercises of the class [Data and Artificial Intelligence for Transportation](https://edu.epfl.ch/coursebook/en/data-and-artificial-intelligence-for-transportation-CIVIL-459) <br/>Cells that need user input have a <font color='blue'> blue title</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites - Google Colab Stuff\n",
    "This version of the notebook is made for running on [google colab](https://colab.research.google.com), a service which offers free GPU runtime for research. However, you can use it on any other device with Cuda technology enabled, just skip the cells in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch on Colab\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Mounting is interactive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Specify path to the folder in your Google Drive.\n",
    "This folder should contain:\n",
    "- The text files,\n",
    "- cnns.py\n",
    "- helpers.py,\n",
    "- embeddings in numpy format\n",
    "- vocabulary in pickle format\n",
    "'''\n",
    "ml_path = '/content/gdrive/My Drive/My_Machine_Learning_Folder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Google Drive path to system path for imports\n",
    "import sys\n",
    "sys.path.insert(0, ml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in your folder\n",
    "import os\n",
    "os.listdir(ml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from helpers import *\n",
    "from cnns import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Load Data and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Specify file paths here:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to data and embeddings\n",
    "path_embeddings = ml_path + 'embeddings200_pretrained.npy'\n",
    "path_vocab = ml_path + 'vocab_pretrained.pkl'\n",
    "path_train_pos = ml_path + 'train_pos.txt'\n",
    "path_train_neg = ml_path + 'train_neg.txt'\n",
    "path_test = ml_path + 'test_data.txt'\n",
    "\n",
    "# Specify paths for savefiles\n",
    "net_filename = ml_path + 'firstnet.pt'\n",
    "stats_filename = ml_path + 'stats_firstnet'\n",
    "submission_filename = ml_path + 'submission_firstnet.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "embeddings = np.load(path_embeddings)\n",
    "# add line of zeroes to the embeddings for empty words\n",
    "embeddings = np.append(np.zeros((1, embeddings.shape[1])), embeddings, axis=0)\n",
    "# load vocabulary\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find maximal tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest = 0\n",
    "for file in [path_train_pos, path_train_neg, path_test]:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            length = len(line.strip().split())\n",
    "            if length > longest:\n",
    "                longest = length          \n",
    "print(\"Longest tweet has {:d} words\".format(longest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each tweet we create an array containing the indexes of the words in\n",
    "the word embedding matrix. All of these vectors have equal length equivalent\n",
    "to the number of words in the longest tweet. If a tweet doesn't fill everything,\n",
    "we pad with 0. this index corresponds to the embedding [0, 0, 0, ... 0, 0].\n",
    "\n",
    "Data type is int32 in order to minimize memory usage.\n",
    "\n",
    "For the labels, we use the value 0 for negative tweets and 1 for positive tweets.\n",
    "'''\n",
    "\n",
    "#initiate empty feature and label lists\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# process positive tweets\n",
    "with open(path_train_pos) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(1)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "\n",
    "# process negative tweets\n",
    "with open(path_train_neg) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        y.append(0)\n",
    "        for word in line.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "        \n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)\n",
    " \n",
    "# Shuffle tweets\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "# We proceed process test tweets in the same way as the training tweets\n",
    "x = []\n",
    "\n",
    "with open(path_test) as f:\n",
    "    for line in f:\n",
    "        tweet = np.int32(np.zeros((longest)))\n",
    "        wordcount = 0\n",
    "        # filter out the IDs and first comma\n",
    "        line_bare = line[(line.index(\",\")+1):]\n",
    "        for word in line_bare.strip().split():\n",
    "            index = vocab.get(word, -1);\n",
    "            # skip words for which we have no embedding\n",
    "            if(index != -1):\n",
    "                tweet[wordcount] = index + 1\n",
    "                wordcount += 1\n",
    "        x.append(tweet)\n",
    "        \n",
    "x_test = np.asarray(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Specify Hyperparameters here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify seed for random weight initialisation\n",
    "torch.manual_seed(1234)\n",
    "# number of channels\n",
    "n_channels = 64\n",
    "# dropout probability (use zero for no dropout)\n",
    "dropout_prob = 0.0\n",
    "# specify the net you want to use\n",
    "net = SimpleConvNet(torch.from_numpy(embeddings).float(), n_channels, dropout_prob).cuda()\n",
    "# decide how many training tweets to use for validation\n",
    "val_prop = 10000\n",
    "batch_size = 1024\n",
    "epochs = 5\n",
    "# defines after how many batches loss and accuracy are displayed\n",
    "print_every = 5\n",
    "# choose optimizer (Adam does fine most of the time)\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get loss criterion of the net and the target type it requires\n",
    "# We let the net class decide on its loss criterion and the type of the targets that this loss criterion requires\n",
    "# Not all nets are compatible with all available criterions\n",
    "\n",
    "criterion, target_type = net.get_criterion()\n",
    "\n",
    "# cut validation data from training data.\n",
    "# convert numpy arrays to torch tensors\n",
    "x_val_torch = torch.from_numpy(x_train[0:val_prop, :])\n",
    "y_val_torch = torch.from_numpy(y_train[0:val_prop]).float()\n",
    "x_train_torch = torch.from_numpy(x_train[val_prop + 1:, :])\n",
    "y_train_torch = torch.from_numpy(y_train[val_prop + 1:]).float()\n",
    "\n",
    "# create batch loaders\n",
    "train_set = utils.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = utils.DataLoader(train_set, batch_size, shuffle=False)\n",
    "val_set = utils.TensorDataset(x_val_torch, y_val_torch)\n",
    "val_loader = utils.DataLoader(val_set, batch_size, shuffle=False)\n",
    "\n",
    "# Lists and counters\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "loss_list = []\n",
    "val_list = []\n",
    "start = time.time()\n",
    "\n",
    "# Iterate through all epochs, predict and backpropagate, update weights\n",
    "for e in range(epochs):\n",
    "    for tweets, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        # converting to Variable is necessary in order to compute the gradient later\n",
    "        inputs = Variable(tweets).cuda()\n",
    "        targets = Variable(labels.to(target_type)).cuda()\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # forward inputs through the net\n",
    "        outputs = net.forward(inputs.long())\n",
    "        # compute loss and gradient\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print loss and accuracy at fixed steps\n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            accuracy = 0\n",
    "            n = 0\n",
    "            # net needs to be set in eval mode to not apply dropout during prediction\n",
    "            net.eval()\n",
    "            for tweets, labels in iter(val_loader):\n",
    "                predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "                accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "                # we cannot use fixed increments of the counter as the last batch can be smaller.\n",
    "                n += labels.data.numpy().size\n",
    "            net.train()\n",
    "            \n",
    "            print(\"Epoch {} / {}\\t\".format(e+1, epochs),\n",
    "                  \"Loss {:.4f}\\t\".format(running_loss / print_every),\n",
    "                  \"Validation accuracy {:.4f}\\t\".format(accuracy / n),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every))\n",
    "            \n",
    "            # append accuracy and loss to the list for plotting\n",
    "            loss_list.append(running_loss / print_every)\n",
    "            val_list.append(accuracy / n)\n",
    "            \n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "            \n",
    "# plot loss and accuracy\n",
    "training_stats = np.column_stack((np.asarray(loss_list), np.asarray(val_list)))\n",
    "plot_val_acc(training_stats)\n",
    "\n",
    "#save net and stats\n",
    "torch.save(net.cpu(), net_filename)\n",
    "np.save(stats_filename, training_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on local validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net needs to be set to eval mode, so it will not use dropout when predicting\n",
    "net.cuda()\n",
    "net.eval()\n",
    "accuracy = 0\n",
    "n = 0\n",
    "for tweets, labels in iter(val_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    accuracy += sum(predictions.data.numpy() == labels.data.numpy())\n",
    "    n += labels.data.numpy().size\n",
    "            \n",
    "print(\"Accuracy on validation set: {:.4f}\".format(accuracy / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create submission file for challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute preditions\n",
    "net.eval()\n",
    "# create a dataloader to iterate over the test data\n",
    "test_loader = utils.DataLoader(torch.from_numpy(x_test), batch_size, shuffle = False)\n",
    "submission_labels = np.zeros((0))\n",
    "for tweets in iter(test_loader):\n",
    "    predictions = net.predict(tweets.long().cuda()).cpu()\n",
    "    #conversion from (0, 1) to (-1, 1)\n",
    "    labels = predictions.data.numpy() * 2 - 1\n",
    "    submission_labels = np.concatenate((submission_labels, labels), axis=0)\n",
    "    \n",
    "# we need to add IDs to meet the submission interface requirements\n",
    "ids = np.arange(len(submission_labels)) + 1\n",
    "create_csv_submission(ids, submission_labels, submission_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
