{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pre_processing_colab.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"jvuFLjZvD_7j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"4c39c11f-d948-4916-ff50-5966e03cc1cf","executionInfo":{"status":"ok","timestamp":1544545506490,"user_tz":-60,"elapsed":10163,"user":{"displayName":"Elias M Poroma W","photoUrl":"https://lh6.googleusercontent.com/-G8GsWXPQKf0/AAAAAAAAAAI/AAAAAAAAAIQ/e4eiL0F1DGk/s64/photo.jpg","userId":"12024984903030299821"}}},"cell_type":"code","source":["!pip3 install torch torchvision\n","!pip3 install -U textblob\n","!pip3 install -U nltk"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages (0.15.2)\n","Requirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.4)\n","Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (3.4.0.3)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n","Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n","Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n"],"name":"stdout"}]},{"metadata":{"id":"8IjZgAYoEEYA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Mount Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AfkbBgNOEIlr","colab_type":"code","colab":{}},"cell_type":"code","source":["# Specify path to files on Google Drive\n","\n","ml_path = '/content/gdrive/My Drive/ML2018/'\n","#ml_path = '/content/gdrive/My Drive/colab/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RYAkth4nELEU","colab_type":"code","colab":{}},"cell_type":"code","source":["# add Google Drive path to system path for imports\n","\n","import sys\n","sys.path.insert(0, ml_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bCrIAz57EOty","colab_type":"code","colab":{}},"cell_type":"code","source":["# List files in Google Drive\n","\n","import os\n","os.listdir(ml_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h2lFI0K5D4UZ","colab_type":"code","colab":{}},"cell_type":"code","source":["% matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import nltk\n","import pickle\n","\n","from datetime import datetime, date, time\n","from textblob import TextBlob\n","from textblob import Word\n","from pre_processing import *\n","from nltk.corpus import stopwords\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mSt3zpa3D4Ue","colab_type":"code","colab":{}},"cell_type":"code","source":["w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","stop = stopwords.words('english')\n","def lemmatize_text(text):\n","    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zp6X3J90D4Ui","colab_type":"code","colab":{}},"cell_type":"code","source":["DATA_FOLDER = './data'\n","\n","tweets_col_names=['text']\n","\n","tweets_dtypes = {'text': str }\n","\n","cleaned_pos = False\n","cleaned_neg = False\n","cleaned_neg_full = True\n","cleaned_pos_full = True\n","cleaned_test = False\n","\n","#freq_word = pd.read_fwf(DATA_FOLDER + '/pre_processed/freq_words_10_withoutpronouns.txt',  names=tweets_col_names,\n","#                                  dtype=tweets_dtypes)\n","#freq_word = list(freq_word.text)\n","# freq_word"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xlp_Qn1WD4Ul","colab_type":"code","colab":{},"outputId":"bd3e4a4b-1f7e-4c47-9a22-272620c3941a"},"cell_type":"code","source":["#verify  flag for data already cleaned\n","if not cleaned_neg :\n","    #read data from .txt to preprocess\n","    tweets_neg = pd.read_fwf(DATA_FOLDER + '/train_neg.txt',  names=tweets_col_names,\n","                                  dtype=tweets_dtypes)\n","    \n","    #increase index to start at 1\n","    tweets_neg.index = tweets_neg.index +1\n","    #remove duplicates\n","    tweets_neg.drop_duplicates(inplace=True)\n","    test = tweets_neg.head(50)\n","    #apply pre_process funtioncs\n","    print(\"emoji\")\n","    # interpret emoji\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: interpret_emoji(x))\n","    print(\"numbers!!\")\n","    #separate number with letters  1234test123 =>  test \n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: split_number_text(x))\n","    print(\"hastg!!\")\n","    #remove hashtag #\n","    tweets_neg['text'] = tweets_neg['text'].str.replace('#', '')\n","    print(\"contraction!!\")\n","    #expand contractions don't => do not\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: expand_contractions(x))\n","    print(\"special cases!!\")\n","        #replace ur by your and other special cases\n","    tweets_neg['text'] = tweets_neg['text'].str.replace(' ur ', ' your ')\n","    tweets_neg['text'] = tweets_neg['text'].str.replace(' u ', ' you ')\n","    tweets_neg['text'] = tweets_neg['text'].str.replace(' cant ', ' can not ')\n","    tweets_neg['text'] = tweets_neg['text'].str.replace(' yourl ', ' your ')\n","    tweets_neg['text'] = tweets_neg['text'].str.replace(' lol ', ' laugh ')\n","    print(\"punctuaction!!\")\n","    #remove punctuaction .........\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_punctuation(x))\n","    print(\"user,url,number!!\")\n","    # remove words user, url, number\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: remove_words(x))\n","    print(\"more letters!\")\n","    # replace more letters haaaaaaaaaappy => happy\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: replace_moreletters(x))\n","\n","    #correct words with textblob\n","    print(\"TEXT BLOB!!\")\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","    # tweets_neg\n","    \n","#     #lemmatize words\n","#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n","    \n","#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n","#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n","# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n","   \n","    #save the file to pickle\n","    print(\"Saving file with preprocessed Tweets\")\n","    pickle_out = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"wb\")\n","    pickle.dump(tweets_neg['text'], pickle_out)\n","    pickle_out.close()\n","    f = open(\"data/pre_processed/tweets_neg_textblob.txt\", \"w\", encoding='utf-8')\n","    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n","    f.close()\n","    print(\"Saved!\")\n","\n","else:\n","    print(\"Already Trained!\")\n","    \n","pickle_in = open(\"data/pre_processed/tweets_neg_textblob.pickle\",\"rb\")\n","tweets_neg_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","tweets_neg_pickle.head()\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["emoji\n","numbers!!\n","hastg!!\n","contraction!!\n","special cases!!\n","punctuaction!!\n","user,url,number!!\n","more letters!\n","TEXT BLOB!!\n"],"name":"stdout"}]},{"metadata":{"id":"fbermCGkD4Uq","colab_type":"code","colab":{}},"cell_type":"code","source":["if not cleaned_pos:\n","    #read data .txt file\n","    tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n","                                  dtype=tweets_dtypes)\n","    # index start at 1\n","    tweets_pos.index = tweets_pos.index + 1\n","    #remove duplicates\n","    tweets_pos.drop_duplicates(inplace=True)\n","    \n","    #apply pre_process funtioncs\n","    # interpret emoji\n","    print(\"emoji\")\n","    # interpret emoji\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: interpret_emoji(x))\n","    print(\"numbers!!\")\n","    #separate number with letters  1234test123 =>  test \n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: split_number_text(x))\n","    print(\"hastg!!\")\n","    #remove hashtag #\n","    tweets_pos['text'] = tweets_pos['text'].str.replace('#', '')\n","    print(\"contraction!!\")\n","    #expand contractions don't => do not\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: expand_contractions(x))\n","    print(\"special cases!!\")\n","        #replace ur by your and other special cases\n","    tweets_pos['text'] = tweets_pos['text'].str.replace(' ur ', ' your ')\n","    tweets_pos['text'] = tweets_pos['text'].str.replace(' u ', ' you ')\n","    tweets_pos['text'] = tweets_pos['text'].str.replace(' cant ', ' can not ')\n","    tweets_pos['text'] = tweets_pos['text'].str.replace(' yourl ', ' your ')\n","    tweets_pos['text'] = tweets_pos['text'].str.replace(' lol ', ' laugh ')\n","    print(\"punctuaction!!\")\n","    #remove punctuaction .........\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_punctuation(x))\n","    print(\"user,url,number!!\")\n","    # remove words user, url, number\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: remove_words(x))\n","    print(\"more letters!\")\n","    # replace more letters haaaaaaaaaappy => happy\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: replace_moreletters(x))\n","\n","    #correct words with textblob\n","    print(\"TEXT BLOB!!\")\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","    \n","#     #lemmatize words\n","#     tweets_neg['text_lema'] = tweets_neg['text'].apply(lemmatize_text)\n","    \n","#     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: ' '.join(x))\n","#     tweets_neg['text_lema'] = tweets_neg['text_lema'].str.replace(',','')\n","# #     tweets_neg['text_lema'] = tweets_neg['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_10))\n","   \n","    #save the file to pickle\n","    print(\"Saving file with preprocessed Tweets\")\n","    pickle_out = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"wb\")\n","    pickle.dump(tweets_neg['text'], pickle_out)\n","    pickle_out.close()\n","    f = open(\"data/pre_processed/tweets_pos_textblob.txt\", \"w\", encoding='utf-8')\n","    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg['text'])))\n","    f.close()\n","    print(\"Saved!\")\n","\n","else:\n","    print(\"Already Trained!\")\n","    \n","pickle_in = open(\"data/pre_processed/tweets_pos_textblob.pickle\",\"rb\")\n","tweets_pos_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","tweets_pos_pickle.head()\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_VaYSe1tD4Ut","colab_type":"code","colab":{}},"cell_type":"code","source":["pickle_in = open(\"data/pre_processed/tweets_pos_removewords.pickle\",\"rb\")\n","tweets_pos_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","tweets_pos_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QhrB-Tc6D4Uw","colab_type":"code","colab":{}},"cell_type":"code","source":["tweets_pos_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w6bDQaCBD4Uz","colab_type":"code","colab":{}},"cell_type":"code","source":["#verify  flag for data already cleaned\n","if not cleaned_neg_full :\n","    #read data from .txt to preprocess\n","    tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n","                                  dtype=tweets_dtypes)\n","    \n","    #increase index to start at 1\n","    tweets_neg_full.index = tweets_neg_full.index +1\n","    #remove duplicates\n","    tweets_neg_full.drop_duplicates(inplace=True)\n","    test = tweets_neg_full.head(50)\n","    #apply pre_process funtioncs\n","    #remove stop words\n","#     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","    print(\"emoji\")\n","    # interpret emoji\n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n","    print(\"split numbers\")\n","    #separate number with letters  1234test123 =>  test \n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n","    print(\"hastag\")\n","    #remove hashtag #\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n","\n","    print(\"expand contractions\")\n","    #expand contractions don't => do not\n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', ' ')\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', ' ')\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', ' ')\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' cant ', ' can not ')\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' your ', ' ')\n","    tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', ' ')\n","    print(\"punctuaction\")\n","        #remove punctuaction\n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n","    print(\"remove words\")\n","    # remove words user, url, number\n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n","    print(\"haaapy to haapy\")\n","    # replace more letters haaaaaaaaaappy => happy\n","    tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n","    \n","    #correct words with textblob\n","    print(\"Correcting with textblob\")\n","    tweets_neg['text'] = tweets_neg['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","    # tweets_neg\n","    print(\"lemmatizing\")\n","    #lemmatize words\n","    tweets_neg_full['text_lema'] = tweets_neg_full['text'].apply(lemmatize_text)\n","   \n","    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: ' '.join(x))\n","    tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].str.replace(',','')\n","#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n","#     tweets_neg_full['text_lema'] = tweets_neg_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_neg_full))\n","    \n","    \n","    #save the file to pickle\n","    print(\"Saving file with preprocessed Tweets\")\n","    pickle_out = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"wb\")\n","    pickle.dump(tweets_neg_full['text_lema'], pickle_out)\n","    pickle_out.close()\n","    f = open(\"data/pre_processed/tweets_neg_full_removewords.txt\", \"w\", encoding='utf-8')\n","    f.write(\"\\n\".join(map(lambda x: str(x), tweets_neg_full['text_lema'])))\n","    f.close()\n","    print(\"Saved!\")\n","else:\n","    print(\"Already Trained!\")\n","    \n","pickle_in = open(\"data/pre_processed/tweets_neg_full_removewords.pickle\",\"rb\")\n","tweets_neg_full_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","tweets_neg_full_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_UJaRFIOD4U3","colab_type":"code","colab":{}},"cell_type":"code","source":["if not cleaned_pos_full:\n","    #read data .txt file\n","    tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n","                                  dtype=tweets_dtypes)\n","    # index start at 1\n","    tweets_pos_full.index = tweets_pos_full.index + 1\n","    #remove duplicates\n","    tweets_pos_full.drop_duplicates(inplace=True)\n","    #apply pre_process funtioncs\n","    #remove stop words\n","#     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","    print(\"Interpreting emojis!!\")\n","     # interpret emoji\n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n","    print(\"Separating Numbers!!\")\n","    #separate number with letters  1234test123 =>  test \n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n","#     print(tweets_pos_full['text'].count())\n","    print(\"Remove Hashtags!!\")\n","    #remove hashtag #\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n","    print(\"Replace ur by your!!\")\n","    \n","    print(\"expand contractions!!\")\n","    #expand contractions don't => do not\n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', ' ')\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', ' ')\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' we ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', ' ')\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', ' ')\n","    tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' your ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', ' ')\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', ' ')\n","    #remove punctuaction\n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n","    print(\"remove words!!\")\n","    # remove words user, url, number\n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n","    print(\"Replace more leeters!!\")\n","    # replace more letters haaaaaaaaaappy => happy\n","    tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n","    \n","#     #remove stop_words\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","     \n","    \n","    \n","#     print(\"Remove most frequent words!!\")\n","#     #remove most frequent words \n","#     freq_max_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[:10]\n","#     freq_max_pos_full = list(freq_max_pos_full.index)\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_max_pos_full))\n","# #     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n","    \n","#     print(\"Remove least frequent words!!\")\n","#     #remove least frequent words\n","#     freq_min_pos_full = pd.Series(' '.join(tweets_pos_full['text']).split()).value_counts()[-10:]\n","#     freq_min_pos_full = list(freq_min_pos_full.index)\n","#     tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n","\n","    #correct words with textblob\n","    # tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","    # tweets_pos\n","    \n","    print(\"Lemmatizing!!\")\n","    #lemmatize words\n","    tweets_pos_full['text_lema'] = tweets_pos_full['text'].apply(lemmatize_text)\n","    \n","   \n","    print(\"joining after lematize\")\n","    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: ' '.join(x))\n","    print(\"removing commas \")\n","    tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].str.replace(',','')\n","#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_word))\n","#     tweets_pos_full['text_lema'] = tweets_pos_full['text_lema'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_min_pos_full))\n","    \n","    #save the file to pickle\n","    print(\"Saving file with preprocessed Tweets\")\n","    pickle_out = open(\"data/pre_processed/tweets_pos_full_removewords.pickle\",\"wb\")\n","    pickle.dump(tweets_pos_full['text_lema'], pickle_out)\n","    pickle_out.close()\n","    f = open(\"data/pre_processed/tweets_pos_full_removewords.txt\", \"w\", encoding='utf-8')\n","    f.write(\"\\n\".join(map(lambda x: str(x), tweets_pos_full['text_lema'])))\n","    f.close()\n","    print(\"Saved!\")\n","else:\n","    print(\"Already Trained!\")\n","pickle_in = open(\"data/pre_processed/tweets_pos_full_removewords.pickle\",\"rb\")\n","tweets_pos_full_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","tweets_pos_full_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ICbQLGAxD4U6","colab_type":"code","colab":{}},"cell_type":"code","source":["if not cleaned_test:  \n","    #read data from .txt\n","    test_data = pd.read_fwf(DATA_FOLDER + '/test_data.txt', header=None)\n","    #index start at 1\n","    test_data.index = test_data.index +1\n","    #remove duplicates\n","    test_data.drop_duplicates(inplace=True)\n","    test_data.drop(columns=[1,2], inplace=True)\n","    test_data.rename(columns={0:'text'}, inplace= True)\n","    \n","    #apply pre_process funtioncs\n","    #remove stop words\n","#     test_data['text'] = test_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","    # interpret emoji\n","    test_data['text'] = test_data['text'].apply(lambda x: interpret_emoji(x))\n","    #separate number with letters  1234test123 => test \n","    test_data['text'] = test_data['text'].apply(lambda x: split_number_text(x))\n","    #remove hashtag #\n","    test_data['text'] = test_data['text'].str.replace('#', '')\n","    #replace ur by your\n","    test_data['text'] = test_data['text'].str.replace('ur', 'your')\n","    #expand contractions don't => do not\n","    test_data['text'] = test_data['text'].apply(lambda x: expand_contractions(x))\n","    test_data['text'] = test_data['text'].str.replace(' ur ', ' ')\n","    test_data['text'] = test_data['text'].str.replace(' u ', ' ')\n","    test_data['text'] = test_data['text'].str.replace(' cant ', ' can not ')\n","    test_data['text'] = test_data['text'].str.replace(' your ', ' ')\n","    test_data['text'] = test_data['text'].str.replace(' yourl ', ' ')\n","\n","            #remove punctuaction\n","    test_data['text'] = test_data['text'].apply(lambda x: remove_punctuation(x))\n"," \n","    # remove words user, url, number\n","    test_data['text'] = test_data['text'].apply(lambda x: remove_words(x))\n","    # replace more letters haaaaaaaaaapy => hapy\n","    test_data['text'] = test_data['text'].apply(lambda x: replace_moreletters(x))\n","\n","    #correct words with textblob\n","    print(\"TEXT BLOB!!!\")\n","    tweets_pos['text'] = tweets_pos['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","    # tweets_pos\n","    \n","\n","        \n","    #lemmatize words\n","    test_data['text_lema'] = test_data['text'].apply(lemmatize_text)\n","    \n","    \n","    test_data['text_lema'] = test_data['text_lema'].apply(lambda x: ' '.join(x))\n","    test_data['text_lema'] = test_data['text_lema'].str.replace(',','')\n","\n","    print(\"Saving file with preprocessed Tweets\")\n","    pickle_out = open(\"data/pre_processed/test_data_removewords.pickle\",\"wb\")\n","    pickle.dump(test_data['text_lema'], pickle_out)\n","    pickle_out.close()\n","    f = open(\"data/pre_processed/test_data_removewords.txt\", \"w\", encoding='utf-8')\n","    f.write(\"\\n\".join(map(lambda x: str(x[0]+1) + ',' + str(x[1]), enumerate(test_data['text_lema']))))\n","    f.close()\n","    print(\"Saved!\")\n","else:\n","    print(\"Already Trained!\")\n","pickle_in = open(\"data/pre_processed/test_data_removewords.pickle\",\"rb\")\n","test_data_pickle = pickle.load(pickle_in)\n","print(\"Opening pickle\")\n","test_data_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hlaJZcmsD4U-","colab_type":"code","colab":{}},"cell_type":"code","source":["test_data_pickle.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bzXMEo50D4VB","colab_type":"code","colab":{}},"cell_type":"code","source":["# test_data_pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iOk53T2MD4VE","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# #read data .txt file\n","# tweets_pos_full = pd.read_fwf(DATA_FOLDER + '/train_pos_full.txt',  names=tweets_col_names,\n","#                               dtype=tweets_dtypes)\n","# # index start at 1\n","# tweets_pos_full.index = tweets_pos_full.index + 1\n","# #remove duplicates\n","# tweets_pos_full.drop_duplicates(inplace=True)\n","# #apply pre_process funtioncs\n","# #remove stop words\n","# #     tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","# print(\"Interpreting emojis!!\")\n","#  # interpret emoji\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: interpret_emoji(x))\n","# print(\"Separating Numbers!!\")\n","# #separate number with letters  1234test123 =>  test \n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: split_number_text(x))\n","# #     print(tweets_pos_full['text'].count())\n","# print(\"Remove Hashtags!!\")\n","# #remove hashtag #\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('#', '')\n","# print(\"Replace ur by your!!\")\n","# #replace ur by your\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace('ur', 'your')\n","# print(\"expand contractions!!\")\n","# #expand contractions don't => do not\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: expand_contractions(x))\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' ur ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' u ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' i ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' he ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' she ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' they ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' it ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' to ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' is ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' and ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' my ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' me ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' the ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' you ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' yourl ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' of ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' for ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' in ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' so ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' this ', '')\n","# tweets_pos_full['text'] = tweets_pos_full['text'].str.replace(' that ', '')\n","# #remove punctuaction\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_punctuation(x))\n","# print(\"remove words!!\")\n","# # remove words user, url, number\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: remove_words(x))\n","# print(\"Replace more leeters!!\")\n","# # replace more letters haaaaaaaaaappy => happy\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: replace_moreletters(x))\n","# tweets_pos_full['text'] = tweets_pos_full['text'].apply(lambda x: one_space(x))\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_X4j7Qi5D4VH","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# #read data from .txt to preprocess\n","# tweets_neg_full = pd.read_fwf(DATA_FOLDER + '/train_neg_full.txt',  names=tweets_col_names,\n","#                               dtype=tweets_dtypes)\n","\n","# #increase index to start at 1\n","# tweets_neg_full.index = tweets_neg_full.index +1\n","# #remove duplicates\n","# tweets_neg_full.drop_duplicates(inplace=True)\n","# test = tweets_neg_full.head(50)\n","# #apply pre_process funtioncs\n","# #remove stop words\n","# #     tweets_neg['text'] = tweets_neg['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","# print(\"emoji\")\n","# # interpret emoji\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: interpret_emoji(x))\n","# print(\"split numbers\")\n","# #separate number with letters  1234test123 =>  test \n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: split_number_text(x))\n","# print(\"hastag\")\n","# #remove hashtag #\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('#', '')\n","# print(\"ur your\")\n","# #replace ur by your\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace('ur', 'your')\n","# print(\"expand contractions\")\n","# #expand contractions don't => do not\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: expand_contractions(x))\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' ur ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' u ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' i ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' he ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' she ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' they ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' it ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' to ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' is ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' and ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' my ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' me ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' the ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' you ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' yourl ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' not ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' of ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' for ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' in ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' so ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' this ', '')\n","# tweets_neg_full['text'] = tweets_neg_full['text'].str.replace(' that ', '')\n","# print(\"punctuaction\")\n","#     #remove punctuaction\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_punctuation(x))\n","# print(\"remove words\")\n","# # remove words user, url, number\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: remove_words(x))\n","# print(\"haaapy to haapy\")\n","# # replace more letters haaaaaaaaaappy => happy\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: replace_moreletters(x))\n","# tweets_neg_full['text'] = tweets_neg_full['text'].apply(lambda x: one_space(x))\n","\n","# #remove stop words\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"46tLN8lBD4VJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# combined = pd.concat([tweets_neg_full,tweets_pos_full])\n","# combined.head()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bipARtVpD4VN","colab_type":"code","colab":{}},"cell_type":"code","source":["# #remove most frequent words \n","# freq_max_total = pd.Series(' '.join(combined['text']).split()).value_counts()[:10]\n","# freq_max_total = list(freq_max_total.index)\n","\n","# #remove least frequent words\n","# freq_min_total = pd.Series(' '.join(combined['text']).split()).value_counts()[-10:]\n","# freq_min_total = list(freq_min_total.index)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ax3K4-_CD4VR","colab_type":"code","colab":{}},"cell_type":"code","source":["# freq_max_total"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QN1gyrvXD4VV","colab_type":"code","colab":{}},"cell_type":"code","source":["# freq_min_total"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K5ubWLZDD4VZ","colab_type":"code","colab":{}},"cell_type":"code","source":["# freq_words = freq_max_total+ freq_min_total\n","# freq_words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bOYEmQl1D4Ve","colab_type":"code","colab":{}},"cell_type":"code","source":["# #save the file to pickle\n","# print(\"Saving most frequent and lest frequent words\")\n","# pickle_out = open(\"data/pre_processed/freq_words_10_withoutpronouns.pickle\",\"wb\")\n","# pickle.dump(freq_words, pickle_out)\n","# pickle_out.close()\n","# f = open(\"data/pre_processed/freq_words_10_withoutpronouns.txt\", \"w\", encoding='utf-8')\n","# f.write(\"\\n\".join(map(lambda x: str(x), freq_words)))\n","# f.close()\n","# print(\"Saved!\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FBiQCvSDD4Vh","colab_type":"code","colab":{}},"cell_type":"code","source":["# pickle_in = open(\"data/pre_processed/freq_words_5.pickle\",\"rb\")\n","# xxxxxx = pickle.load(pickle_in)\n","# print(\"Opening pickle\")\n","# xxxxxx"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lR3lJltcD4Vj","colab_type":"code","colab":{}},"cell_type":"code","source":["# tweets_pos = pd.read_fwf(DATA_FOLDER + '/train_pos.txt',  names=tweets_col_names,\n","#                               dtype=tweets_dtypes)\n","# # index start at 1\n","# tweets_pos.index = tweets_pos.index + 1\n","# #remove duplicates\n","# tweets_pos.drop_duplicates(inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zsv3SIs9D4Vm","colab_type":"code","colab":{}},"cell_type":"code","source":["# tweets_neg_full_pickle.str.contains('like').sum()\n","# tweets_pos.text.str.contains('like').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7V952q6OD4Vp","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# tweets_pos.replace(freq_words_20,' ', inplace=True)\n","# tweets_pos.text.str.contains('like').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MR5wlZaGD4Vs","colab_type":"code","colab":{}},"cell_type":"code","source":["# tweets_pos['text'] = tweets_pos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n","# tweets_pos.text.str.contains(' like ').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KgyS_cZ0D4Vu","colab_type":"code","colab":{}},"cell_type":"code","source":["# freq_words_20"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Uq--YorjD4Vw","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","    \n","# pickle_in = open(\"data/pre_processed/tweets_neg_freqwords20.pickle\",\"rb\")\n","# tweets_neg_freqwords20 = pickle.load(pickle_in)\n","# tweets_neg_freqwords20.str.contains('yourl').sum()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lydOhawwD4Vz","colab_type":"code","colab":{}},"cell_type":"code","source":["# tweets_neg_freqwords20.apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words_20))\n","# tweets_neg_freqwords20.str.contains('yourl').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6kcJyxqBD4V3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"q_1MbCJrD4V6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IjB88jnfD4V8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"KsoK5kM4D4V-","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3DccA4yQD4WB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vzg6NhlPD4WD","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Iif3n57gD4WG","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"ez59R_qtD4WI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"RmAjSNoHD4WJ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"M7yxYNo9D4WM","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"4MlYOB_SD4WO","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"wHaSUGn4D4WP","colab_type":"code","colab":{}},"cell_type":"code","source":["test['text'] = test['text'].apply(lambda x: ''.join(TextBlob(x).correct()))\n","test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6OeJkuVVD4WR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}